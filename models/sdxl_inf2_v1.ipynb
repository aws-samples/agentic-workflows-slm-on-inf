{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c1b5e70",
   "metadata": {},
   "source": [
    "# SDXL on INF2    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf46a1",
   "metadata": {},
   "source": [
    "In this notebook, we deploy a Stable Diffusion XL model using an Inferentia2 instance and optimum-neuron on Amazon SageMaker. Optimum Neuron is the interface betweeen the Transfomers library and AWS Purpose Built Accelerators, including AWS Inferentia.\n",
    "\n",
    "Reference: https://www.philschmid.de/inferentia2-stable-diffusion-xl     \n",
    "Author: Yunfei Bai     \n",
    "Date: 2024/09/11   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0648579e",
   "metadata": {},
   "source": [
    "## Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f470ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"optimum-neuron\" \"diffusers\" \n",
    "!pip install -U \"sagemaker>=2.197.0\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0971797a",
   "metadata": {},
   "source": [
    "Note: you may need to restart the kernel to use updated packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878c1226",
   "metadata": {},
   "source": [
    "## Save compiled model to local directory, and download a snapshot of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddebd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    " \n",
    "# compiled model id\n",
    "compiled_model_id = \"aws-neuron/stable-diffusion-xl-base-1-0-1024x1024\"\n",
    " \n",
    "# save compiled model to local directory\n",
    "save_directory = \"sdxl_neuron\"\n",
    "\n",
    "# Downloads our compiled model from the HuggingFace Hub\n",
    "# using the revision as neuron version reference\n",
    "# and makes sure we exlcude the symlink files and \"hidden\" files, like .DS_Store, .gitignore, etc.\n",
    "snapshot_download(compiled_model_id, revision=\"2.15.0\", local_dir=save_directory, local_dir_use_symlinks=False, allow_patterns=[\"[!.]*.*\"])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3575504",
   "metadata": {},
   "source": [
    "## Create code directory and inferency.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c7a492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create code directory in our model directory\n",
    "!mkdir {save_directory}/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6809a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {save_directory}/code/inference.py\n",
    "\n",
    "import os\n",
    "# Assign two neuron cores per worker\n",
    "os.environ[\"NEURON_RT_NUM_CORES\"] = \"2\"\n",
    "import torch\n",
    "import torch_neuronx\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from optimum.neuron import NeuronStableDiffusionXLPipeline\n",
    " \n",
    " \n",
    "def model_fn(model_dir):\n",
    "    # Load local converted model into pipeline\n",
    "    pipeline = NeuronStableDiffusionXLPipeline.from_pretrained(model_dir, device_ids=[0, 1])\n",
    "    return pipeline\n",
    " \n",
    " \n",
    "def predict_fn(data, pipeline):\n",
    "    # Extract prompt from data\n",
    "    prompt = data.pop(\"inputs\", data)\n",
    " \n",
    "    parameters = data.pop(\"parameters\", None)\n",
    " \n",
    "    if parameters is not None:\n",
    "        generated_images = pipeline(prompt, **parameters)[\"images\"]\n",
    "    else:\n",
    "        generated_images = pipeline(prompt)[\"images\"]\n",
    " \n",
    "    # Convert image into base64 string\n",
    "    encoded_images = []\n",
    "    for image in generated_images:\n",
    "        buffered = BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\")\n",
    "        encoded_images.append(base64.b64encode(buffered.getvalue()).decode())\n",
    " \n",
    "    # Always return the first image\n",
    "    return {\"generated_images\": encoded_images}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f63e0f",
   "metadata": {},
   "source": [
    "## Configure SageMaker resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401dc6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# Create an Amazon Sagemaker session bucket for uploading data, models and logs\n",
    "# Amazon Sagemaker will automatically create this bucket if it does not exist\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # If a bucket name is not provided, set to default bucket\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    " \n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    " \n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    " \n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "assert sess.boto_region_name in [\"us-east-2\", \"us-east-1\"] , \"region must be us-east-2 or us-west-2, due to instance availability\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c01272c",
   "metadata": {},
   "source": [
    "## Create tar file with model artifacts and scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bed63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model.tar.gz archive with all the model artifacts and the inference.py script.\n",
    "%cd {save_directory}\n",
    "!tar zcvf model.tar.gz *\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7daa930",
   "metadata": {},
   "source": [
    "## Upload model tar file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6d2052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    " \n",
    "# Create s3 uri\n",
    "s3_model_path = f\"s3://{sess.default_bucket()}/neuronx/sdxl\"\n",
    " \n",
    "# Upload model.tar.gz\n",
    "s3_model_uri = S3Uploader.upload(local_path=f\"{save_directory}/model.tar.gz\", desired_s3_uri=s3_model_path)\n",
    "print(f\"model artifcats uploaded to {s3_model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa70d29",
   "metadata": {},
   "source": [
    "## Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c391b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    " \n",
    "# Create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=s3_model_uri,        # path to your model.tar.gz on s3\n",
    "   role=role,                      # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.34.1\",  # transformers version used\n",
    "   pytorch_version=\"1.13.1\",       # pytorch version used\n",
    "   py_version='py310',             # python version used\n",
    "   model_server_workers=1,         # number of workers for the model server\n",
    ")\n",
    " \n",
    "# Deploy the endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,      # number of instances\n",
    "    instance_type=\"ml.inf2.xlarge\", # AWS Inferentia Instance\n",
    "    volume_size = 100\n",
    ")\n",
    "# Ignore the \"Your model is not compiled. Please compile your model before using Inferentia.\" warning, we already compiled our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05000ce0",
   "metadata": {},
   "source": [
    "## Invoke the model with a sample prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e0a49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from IPython.display import display\n",
    "import base64\n",
    " \n",
    "# Helper decoder\n",
    "def decode_base64_image(image_string):\n",
    "  base64_image = base64.b64decode(image_string)\n",
    "  buffer = BytesIO(base64_image)\n",
    "  return Image.open(buffer)\n",
    " \n",
    "# Display PIL images as grid\n",
    "def display_image(image=None,width=500,height=500):\n",
    "    img = image.resize((width, height))\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06904515",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A dog trying catch a flying pizza at a street corner, comic book, well lit, night time\"\n",
    " \n",
    "# Run prediction\n",
    "response = predictor.predict(data={\n",
    "  \"inputs\": prompt,\n",
    "  \"parameters\": {\n",
    "    \"num_inference_steps\" : 20,\n",
    "    \"negative_prompt\" : \"disfigured, ugly, deformed\"\n",
    "    }\n",
    "  }\n",
    ")\n",
    " \n",
    "# Decode and display image\n",
    "display_image(decode_base64_image(response[\"generated_images\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f41053",
   "metadata": {},
   "source": [
    "## Clean up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee49bbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501d61a6-2fd8-4001-909b-0172d3188e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e6d895-6f29-4798-830f-aafd89be921f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}